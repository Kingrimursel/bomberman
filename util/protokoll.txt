STATISTISCHE METHODEN
---------------------

Für die bedachte und nachhaltige Entwicklung unserer Agenten hat es sich als äußerst wichtig herausgestellt, das Training in den unterschiedlichen Stadien mit Statistik zu analysieren. Die visuelle Darstellung ermöglicht eine vereinfachte Analyse und zielgerichtete Behebung von Fehlern.

Um die notwendige Statistik zu generieren, auf die wir in den nächsten Absätzen noch genauer eingehen werden, haben wir uns dazu entschieden wichtige Spielinformationen wie beispielsweise den score am Ende eines Spiels oder die am Ende einer Runde ausgeführten events zu loggen, und die log-files dann mit einem script (zunächst mit Hilfe von Bash, dann von Python) zu analysieren und die wichtigen Informationen zu extrahieren. Am Ende einer Trainingssession werden dann die entsprechenden Grafiken erstellt. Dabei handelt es sich unter anderem um eine Reihe von PyCharts, die für jedes Spiel die action-distribution darstellen und so einen schnellen Überblick über die relativen action-Häufigkeiten ermöglichen (figure ???). So lässt sich beispielsweise beobachten, dass die relative Anzahl der illegal moves mit fortschreitendem training abnimmt (figure ???)

Da jedoch nicht nur die relative Häufigkeit von bestimmten Aktionen interessant sein kann, sondern auch das Verhalten der absoluten Häufigkeiten im Verlauf mehrerer Spiele, werden auch hier entsprechende Grafiken erstellt. In Figure ??? wird eine Übersicht dieser Grafiken gezeigt.

Um den Erfolg unseres Agenten zu tracken werden außerdem Grafiken erstellt, die Platzierung und finalen score zeigen.


Dies vorgestellten Statistiken verwenden wir dann auch, um Hyperparameter optimization zu betreiben. Wir optimieren die ... alpha sowie die ... gamma. Für unterschiedliche Konstellationen werden wir Platzierung und finalen score nach einer trainingsperiode und dann einer testperiode untersuchen, um die optimalen Parameter zu finden.

TODO:
---------------
Vergleichen: own_coin, die beiden anderen Agenten mit Hilfe der Grafiken. Wie die sich untereinander unterscheiden.#

Dass wir zuerst deterministisch trainieren sieht man Anfangs in den Statistiken, da wir dort schon gute Plätze belegen.
---------------

TRAININGSMETHODEN
-----------------

TODO:
------------------------
Trainingsmethoden beschreiben. Also erst deterministisch, dann exploration phase und dann normale exploitation.
------------------------


HYPERPARAMETER OPTIMIZATION
---------------------------

Durch Berücksichtigung der vierzähligen Rotationssymmetrie können wir die Q-Table mit vierfacher Geschwindigkeit updaten. Dies ermöglich ein vierfach schnelleres Training, sodass wir uns dafür entschieden haben mit brute force alle möglichen Kombinationen von gamma und alpha durchzuprobieren, wobei gamma und alpha auf einem grid auf dem Intervall [0, 1] liegen dessen spacing wir vorher festlegen.

Wir trainieren nun für jede der berücksichtigten hyperparemeter-Konstellationen ein Modell und testen dieses danach über eine feste anzahl von Spielen. Dann berechnen wir die durchschnittliche Positionierung des Agenten während der tests und wählen jene Hyperparameter, die die durchschnittliche Positionierund minimieren. Im Falle einer Ambiguität wählen wir die Konstellation, die den durchschnittlichen score maximiert.

Hier gibt es theoretisch noch Verbesserungsbedarf. Anstatt den durchschnittlichen score nur im Falle eines Unentschiedens nach mittlerer Positionierung zu berücksichtigen, könnte man eine gewisse range an durchschnittlichen Positionierungen definieren, die dann auf den durchschnittlichen score untersucht werden. Denn eine sehr wenig schlechtere durchschnittliche Positionierung kann durchaus Zufall sein, obwohl der er erreichte score überdurchschnittlich groß ist. Aus Zeitgründen haben wir uns diesem Problem zunächst nicht gewidmet, da es andere dringendere Probleme gab.



